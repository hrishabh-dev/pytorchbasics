{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNhBNyWXOuQRD4MSZwsa9CM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression from scratch using Pytorch"
      ],
      "metadata": {
        "id": "fE1NX23wSF3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjDc1cMMR3MG"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "i4HncZyTR6z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making training data\n",
        "# Input --> (temperature,rainfall,humidity)---> Yield  of apple and oranges crop"
      ],
      "metadata": {
        "id": "6CHr6cC-SC2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.array([\n",
        "    [56,70,45],\n",
        "    [44,56,56],\n",
        "    [34,54,56],\n",
        "    [78,67,67],\n",
        "    [34,76,56]\n",
        "\n",
        "],dtype='float32')"
      ],
      "metadata": {
        "id": "Nwxjp8JHSfea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target - (apples,oranges)\n",
        "target=np.array([\n",
        "    [23,45] ,\n",
        "    [45,65],\n",
        "    [67,43],\n",
        "    [23,32],\n",
        "    [12,21]\n",
        "],dtype='float32')"
      ],
      "metadata": {
        "id": "cbUTwOA1TJTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert inputs and target to tensors\n",
        "inputs=torch.from_numpy(inputs)\n",
        "target=torch.from_numpy(target)\n",
        "print(inputs)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTUV5uibTZZw",
        "outputId": "dae2e709-1aab-4077-f38c-764480e124a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[56., 70., 45.],\n",
            "        [44., 56., 56.],\n",
            "        [34., 54., 56.],\n",
            "        [78., 67., 67.],\n",
            "        [34., 76., 56.]])\n",
            "tensor([[23., 45.],\n",
            "        [45., 65.],\n",
            "        [67., 43.],\n",
            "        [23., 32.],\n",
            "        [12., 21.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights and biases\n",
        "w=torch.randn(2,3,requires_grad=True)\n",
        "b=torch.randn(2,requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGKMvwUuTl_7",
        "outputId": "b3241e41-26b7-47a6-c277-3418f2160f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7506, -0.6299, -1.1776],\n",
            "        [-1.0079, -0.8620,  0.7277]], requires_grad=True)\n",
            "tensor([-0.2533, -0.2154], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def model(x):\n",
        "  return x @ w.t()+b"
      ],
      "metadata": {
        "id": "mVzlO9zlUZJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions\n",
        "preds=model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cWTNl1DVrE4",
        "outputId": "6d20b71e-3187-4988-9760-02e79dc6e47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-55.3038, -84.2543],\n",
            "        [-68.4453, -52.0861],\n",
            "        [-74.6914, -40.2827],\n",
            "        [-62.8073, -87.8332],\n",
            "        [-88.5495, -59.2472]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function - mse  (mean_squared_error )\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "OUn-6onIVvgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def MSE(actual,predicted):\n",
        "\n",
        "   diff = torch.mean((actual - predicted)**2)\n",
        "   return diff"
      ],
      "metadata": {
        "id": "YRWWkm9kWK9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WymKu9fyZNVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print loss\n",
        "loss=MSE(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b92YiMdyWumy",
        "outputId": "578f97cb-46bb-4176-cde5-44deb1b68e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11470.2305, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute gradient\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "cFu0fHiSXLR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHVTc5YGYtzy",
        "outputId": "dc935f66-2a7f-41a5-c4f2-fd7e65d6850c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7506, -0.6299, -1.1776],\n",
            "        [-1.0079, -0.8620,  0.7277]], requires_grad=True)\n",
            "tensor([[-4861.1533, -6575.2778, -5838.2378],\n",
            "        [-5459.4062, -6845.8989, -5911.9521]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqXpEKXda4l0",
        "outputId": "44317ca6-5ce8-4272-c8ca-5d04a0a4c569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2533, -0.2154], requires_grad=True)\n",
            "tensor([-207.9189, -211.8814])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reset gradient\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlYNqvr8bFZn",
        "outputId": "9a94eb0c-e0ef-4e83-a63c-633abf512885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust weight and grads\n",
        "with torch.no_grad():\n",
        "  w-=w.grad*1e-5\n",
        "  b-=b.grad*1e-5\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "ugvNbHf6j9Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBZ9awZamded",
        "outputId": "179f7dcd-583e-434f-d3e2-89013ed68f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7506, -0.6299, -1.1776],\n",
            "        [-1.0079, -0.8620,  0.7277]], requires_grad=True)\n",
            "tensor([-0.2533, -0.2154], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training for multiple  epochs\n",
        "for i in range(400):\n",
        "  preds=model(inputs)\n",
        "  loss=MSE(target,preds)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w-=w.grad*1e-5\n",
        "    b-=b.grad*1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f\"Epochs{i} and loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkvOg0kImpbd",
        "outputId": "9e82bb68-d64c-4e64-adb3-982cd2ac537b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs0 and loss 313.94329833984375\n",
            "Epochs1 and loss 313.88751220703125\n",
            "Epochs2 and loss 313.8318176269531\n",
            "Epochs3 and loss 313.7761535644531\n",
            "Epochs4 and loss 313.7205505371094\n",
            "Epochs5 and loss 313.66510009765625\n",
            "Epochs6 and loss 313.60968017578125\n",
            "Epochs7 and loss 313.55438232421875\n",
            "Epochs8 and loss 313.4991149902344\n",
            "Epochs9 and loss 313.44403076171875\n",
            "Epochs10 and loss 313.388916015625\n",
            "Epochs11 and loss 313.3338317871094\n",
            "Epochs12 and loss 313.2789611816406\n",
            "Epochs13 and loss 313.22406005859375\n",
            "Epochs14 and loss 313.1693420410156\n",
            "Epochs15 and loss 313.1146545410156\n",
            "Epochs16 and loss 313.05999755859375\n",
            "Epochs17 and loss 313.0054626464844\n",
            "Epochs18 and loss 312.9510192871094\n",
            "Epochs19 and loss 312.8966369628906\n",
            "Epochs20 and loss 312.84234619140625\n",
            "Epochs21 and loss 312.78814697265625\n",
            "Epochs22 and loss 312.73394775390625\n",
            "Epochs23 and loss 312.679931640625\n",
            "Epochs24 and loss 312.62591552734375\n",
            "Epochs25 and loss 312.572021484375\n",
            "Epochs26 and loss 312.5181579589844\n",
            "Epochs27 and loss 312.46441650390625\n",
            "Epochs28 and loss 312.4107360839844\n",
            "Epochs29 and loss 312.35711669921875\n",
            "Epochs30 and loss 312.3036193847656\n",
            "Epochs31 and loss 312.25018310546875\n",
            "Epochs32 and loss 312.19677734375\n",
            "Epochs33 and loss 312.14349365234375\n",
            "Epochs34 and loss 312.09027099609375\n",
            "Epochs35 and loss 312.0370788574219\n",
            "Epochs36 and loss 311.9840393066406\n",
            "Epochs37 and loss 311.9309997558594\n",
            "Epochs38 and loss 311.87811279296875\n",
            "Epochs39 and loss 311.8252868652344\n",
            "Epochs40 and loss 311.7724609375\n",
            "Epochs41 and loss 311.7198181152344\n",
            "Epochs42 and loss 311.66717529296875\n",
            "Epochs43 and loss 311.6145935058594\n",
            "Epochs44 and loss 311.5621643066406\n",
            "Epochs45 and loss 311.5097351074219\n",
            "Epochs46 and loss 311.4574279785156\n",
            "Epochs47 and loss 311.4052429199219\n",
            "Epochs48 and loss 311.35296630859375\n",
            "Epochs49 and loss 311.3009338378906\n",
            "Epochs50 and loss 311.2488708496094\n",
            "Epochs51 and loss 311.1969299316406\n",
            "Epochs52 and loss 311.1450500488281\n",
            "Epochs53 and loss 311.09326171875\n",
            "Epochs54 and loss 311.04150390625\n",
            "Epochs55 and loss 310.9898376464844\n",
            "Epochs56 and loss 310.93829345703125\n",
            "Epochs57 and loss 310.88677978515625\n",
            "Epochs58 and loss 310.8353271484375\n",
            "Epochs59 and loss 310.78387451171875\n",
            "Epochs60 and loss 310.732666015625\n",
            "Epochs61 and loss 310.6814270019531\n",
            "Epochs62 and loss 310.63031005859375\n",
            "Epochs63 and loss 310.57916259765625\n",
            "Epochs64 and loss 310.5282287597656\n",
            "Epochs65 and loss 310.47723388671875\n",
            "Epochs66 and loss 310.4264221191406\n",
            "Epochs67 and loss 310.3756103515625\n",
            "Epochs68 and loss 310.3249206542969\n",
            "Epochs69 and loss 310.27423095703125\n",
            "Epochs70 and loss 310.22369384765625\n",
            "Epochs71 and loss 310.17315673828125\n",
            "Epochs72 and loss 310.12274169921875\n",
            "Epochs73 and loss 310.0723876953125\n",
            "Epochs74 and loss 310.0221252441406\n",
            "Epochs75 and loss 309.97186279296875\n",
            "Epochs76 and loss 309.9217529296875\n",
            "Epochs77 and loss 309.87164306640625\n",
            "Epochs78 and loss 309.8216247558594\n",
            "Epochs79 and loss 309.771728515625\n",
            "Epochs80 and loss 309.7218322753906\n",
            "Epochs81 and loss 309.6720275878906\n",
            "Epochs82 and loss 309.6223449707031\n",
            "Epochs83 and loss 309.5726623535156\n",
            "Epochs84 and loss 309.5231018066406\n",
            "Epochs85 and loss 309.47357177734375\n",
            "Epochs86 and loss 309.4241027832031\n",
            "Epochs87 and loss 309.37469482421875\n",
            "Epochs88 and loss 309.3254089355469\n",
            "Epochs89 and loss 309.27618408203125\n",
            "Epochs90 and loss 309.22698974609375\n",
            "Epochs91 and loss 309.17791748046875\n",
            "Epochs92 and loss 309.1288757324219\n",
            "Epochs93 and loss 309.0799255371094\n",
            "Epochs94 and loss 309.0309753417969\n",
            "Epochs95 and loss 308.982177734375\n",
            "Epochs96 and loss 308.93341064453125\n",
            "Epochs97 and loss 308.884765625\n",
            "Epochs98 and loss 308.8360900878906\n",
            "Epochs99 and loss 308.78753662109375\n",
            "Epochs100 and loss 308.73907470703125\n",
            "Epochs101 and loss 308.690673828125\n",
            "Epochs102 and loss 308.6422424316406\n",
            "Epochs103 and loss 308.5940246582031\n",
            "Epochs104 and loss 308.5457458496094\n",
            "Epochs105 and loss 308.4975891113281\n",
            "Epochs106 and loss 308.44952392578125\n",
            "Epochs107 and loss 308.4014892578125\n",
            "Epochs108 and loss 308.353515625\n",
            "Epochs109 and loss 308.3056335449219\n",
            "Epochs110 and loss 308.25787353515625\n",
            "Epochs111 and loss 308.2100524902344\n",
            "Epochs112 and loss 308.16241455078125\n",
            "Epochs113 and loss 308.11480712890625\n",
            "Epochs114 and loss 308.06719970703125\n",
            "Epochs115 and loss 308.0198059082031\n",
            "Epochs116 and loss 307.9723205566406\n",
            "Epochs117 and loss 307.92498779296875\n",
            "Epochs118 and loss 307.8777160644531\n",
            "Epochs119 and loss 307.8304748535156\n",
            "Epochs120 and loss 307.7833557128906\n",
            "Epochs121 and loss 307.7362365722656\n",
            "Epochs122 and loss 307.68914794921875\n",
            "Epochs123 and loss 307.64227294921875\n",
            "Epochs124 and loss 307.59527587890625\n",
            "Epochs125 and loss 307.54852294921875\n",
            "Epochs126 and loss 307.50177001953125\n",
            "Epochs127 and loss 307.4550476074219\n",
            "Epochs128 and loss 307.4084167480469\n",
            "Epochs129 and loss 307.36187744140625\n",
            "Epochs130 and loss 307.3153381347656\n",
            "Epochs131 and loss 307.2688903808594\n",
            "Epochs132 and loss 307.22247314453125\n",
            "Epochs133 and loss 307.17620849609375\n",
            "Epochs134 and loss 307.12994384765625\n",
            "Epochs135 and loss 307.0837707519531\n",
            "Epochs136 and loss 307.03759765625\n",
            "Epochs137 and loss 306.9916076660156\n",
            "Epochs138 and loss 306.9455871582031\n",
            "Epochs139 and loss 306.8996276855469\n",
            "Epochs140 and loss 306.85382080078125\n",
            "Epochs141 and loss 306.8080139160156\n",
            "Epochs142 and loss 306.76226806640625\n",
            "Epochs143 and loss 306.716552734375\n",
            "Epochs144 and loss 306.67095947265625\n",
            "Epochs145 and loss 306.6253967285156\n",
            "Epochs146 and loss 306.5799255371094\n",
            "Epochs147 and loss 306.5344543457031\n",
            "Epochs148 and loss 306.48907470703125\n",
            "Epochs149 and loss 306.44378662109375\n",
            "Epochs150 and loss 306.3985290527344\n",
            "Epochs151 and loss 306.3533935546875\n",
            "Epochs152 and loss 306.3082275390625\n",
            "Epochs153 and loss 306.26318359375\n",
            "Epochs154 and loss 306.21826171875\n",
            "Epochs155 and loss 306.1733093261719\n",
            "Epochs156 and loss 306.1283874511719\n",
            "Epochs157 and loss 306.0836486816406\n",
            "Epochs158 and loss 306.03887939453125\n",
            "Epochs159 and loss 305.99420166015625\n",
            "Epochs160 and loss 305.9495849609375\n",
            "Epochs161 and loss 305.90496826171875\n",
            "Epochs162 and loss 305.8605041503906\n",
            "Epochs163 and loss 305.81610107421875\n",
            "Epochs164 and loss 305.77166748046875\n",
            "Epochs165 and loss 305.7273864746094\n",
            "Epochs166 and loss 305.6831359863281\n",
            "Epochs167 and loss 305.638916015625\n",
            "Epochs168 and loss 305.59478759765625\n",
            "Epochs169 and loss 305.55072021484375\n",
            "Epochs170 and loss 305.5067443847656\n",
            "Epochs171 and loss 305.4627380371094\n",
            "Epochs172 and loss 305.41888427734375\n",
            "Epochs173 and loss 305.37506103515625\n",
            "Epochs174 and loss 305.3312072753906\n",
            "Epochs175 and loss 305.2875061035156\n",
            "Epochs176 and loss 305.2438659667969\n",
            "Epochs177 and loss 305.2002868652344\n",
            "Epochs178 and loss 305.1567077636719\n",
            "Epochs179 and loss 305.1132507324219\n",
            "Epochs180 and loss 305.0698547363281\n",
            "Epochs181 and loss 305.02655029296875\n",
            "Epochs182 and loss 304.98321533203125\n",
            "Epochs183 and loss 304.9399719238281\n",
            "Epochs184 and loss 304.8967590332031\n",
            "Epochs185 and loss 304.85369873046875\n",
            "Epochs186 and loss 304.81060791015625\n",
            "Epochs187 and loss 304.767578125\n",
            "Epochs188 and loss 304.724609375\n",
            "Epochs189 and loss 304.6817321777344\n",
            "Epochs190 and loss 304.63897705078125\n",
            "Epochs191 and loss 304.5962219238281\n",
            "Epochs192 and loss 304.5534362792969\n",
            "Epochs193 and loss 304.51080322265625\n",
            "Epochs194 and loss 304.46820068359375\n",
            "Epochs195 and loss 304.4256896972656\n",
            "Epochs196 and loss 304.38323974609375\n",
            "Epochs197 and loss 304.34075927734375\n",
            "Epochs198 and loss 304.2983703613281\n",
            "Epochs199 and loss 304.256103515625\n",
            "Epochs200 and loss 304.2138977050781\n",
            "Epochs201 and loss 304.17169189453125\n",
            "Epochs202 and loss 304.1295471191406\n",
            "Epochs203 and loss 304.08746337890625\n",
            "Epochs204 and loss 304.04547119140625\n",
            "Epochs205 and loss 304.00347900390625\n",
            "Epochs206 and loss 303.9615173339844\n",
            "Epochs207 and loss 303.919677734375\n",
            "Epochs208 and loss 303.87786865234375\n",
            "Epochs209 and loss 303.836181640625\n",
            "Epochs210 and loss 303.79449462890625\n",
            "Epochs211 and loss 303.75286865234375\n",
            "Epochs212 and loss 303.7112731933594\n",
            "Epochs213 and loss 303.66973876953125\n",
            "Epochs214 and loss 303.6283264160156\n",
            "Epochs215 and loss 303.5869445800781\n",
            "Epochs216 and loss 303.54559326171875\n",
            "Epochs217 and loss 303.5043640136719\n",
            "Epochs218 and loss 303.46307373046875\n",
            "Epochs219 and loss 303.4219055175781\n",
            "Epochs220 and loss 303.3807678222656\n",
            "Epochs221 and loss 303.3397216796875\n",
            "Epochs222 and loss 303.2987060546875\n",
            "Epochs223 and loss 303.2577819824219\n",
            "Epochs224 and loss 303.2168884277344\n",
            "Epochs225 and loss 303.1760559082031\n",
            "Epochs226 and loss 303.1352233886719\n",
            "Epochs227 and loss 303.094482421875\n",
            "Epochs228 and loss 303.0538635253906\n",
            "Epochs229 and loss 303.01324462890625\n",
            "Epochs230 and loss 302.97265625\n",
            "Epochs231 and loss 302.9321594238281\n",
            "Epochs232 and loss 302.8917541503906\n",
            "Epochs233 and loss 302.85125732421875\n",
            "Epochs234 and loss 302.8109130859375\n",
            "Epochs235 and loss 302.7705993652344\n",
            "Epochs236 and loss 302.73040771484375\n",
            "Epochs237 and loss 302.690185546875\n",
            "Epochs238 and loss 302.65008544921875\n",
            "Epochs239 and loss 302.6100158691406\n",
            "Epochs240 and loss 302.5699768066406\n",
            "Epochs241 and loss 302.52996826171875\n",
            "Epochs242 and loss 302.49005126953125\n",
            "Epochs243 and loss 302.4501647949219\n",
            "Epochs244 and loss 302.41046142578125\n",
            "Epochs245 and loss 302.37066650390625\n",
            "Epochs246 and loss 302.3309020996094\n",
            "Epochs247 and loss 302.29132080078125\n",
            "Epochs248 and loss 302.251708984375\n",
            "Epochs249 and loss 302.212158203125\n",
            "Epochs250 and loss 302.17266845703125\n",
            "Epochs251 and loss 302.13323974609375\n",
            "Epochs252 and loss 302.0938415527344\n",
            "Epochs253 and loss 302.05450439453125\n",
            "Epochs254 and loss 302.0152587890625\n",
            "Epochs255 and loss 301.9760437011719\n",
            "Epochs256 and loss 301.9368896484375\n",
            "Epochs257 and loss 301.89776611328125\n",
            "Epochs258 and loss 301.858642578125\n",
            "Epochs259 and loss 301.8196716308594\n",
            "Epochs260 and loss 301.7806701660156\n",
            "Epochs261 and loss 301.74176025390625\n",
            "Epochs262 and loss 301.70294189453125\n",
            "Epochs263 and loss 301.66412353515625\n",
            "Epochs264 and loss 301.6253356933594\n",
            "Epochs265 and loss 301.5866394042969\n",
            "Epochs266 and loss 301.54803466796875\n",
            "Epochs267 and loss 301.5093688964844\n",
            "Epochs268 and loss 301.4708251953125\n",
            "Epochs269 and loss 301.4323425292969\n",
            "Epochs270 and loss 301.39385986328125\n",
            "Epochs271 and loss 301.35552978515625\n",
            "Epochs272 and loss 301.31719970703125\n",
            "Epochs273 and loss 301.27886962890625\n",
            "Epochs274 and loss 301.2406921386719\n",
            "Epochs275 and loss 301.20245361328125\n",
            "Epochs276 and loss 301.1643371582031\n",
            "Epochs277 and loss 301.1262512207031\n",
            "Epochs278 and loss 301.08819580078125\n",
            "Epochs279 and loss 301.0501708984375\n",
            "Epochs280 and loss 301.0122375488281\n",
            "Epochs281 and loss 300.9743347167969\n",
            "Epochs282 and loss 300.9365539550781\n",
            "Epochs283 and loss 300.8987121582031\n",
            "Epochs284 and loss 300.861083984375\n",
            "Epochs285 and loss 300.8233642578125\n",
            "Epochs286 and loss 300.7857360839844\n",
            "Epochs287 and loss 300.74810791015625\n",
            "Epochs288 and loss 300.7106018066406\n",
            "Epochs289 and loss 300.6730651855469\n",
            "Epochs290 and loss 300.63568115234375\n",
            "Epochs291 and loss 300.59832763671875\n",
            "Epochs292 and loss 300.56097412109375\n",
            "Epochs293 and loss 300.523681640625\n",
            "Epochs294 and loss 300.4864807128906\n",
            "Epochs295 and loss 300.44927978515625\n",
            "Epochs296 and loss 300.4121398925781\n",
            "Epochs297 and loss 300.3750915527344\n",
            "Epochs298 and loss 300.3379821777344\n",
            "Epochs299 and loss 300.3010559082031\n",
            "Epochs300 and loss 300.2640380859375\n",
            "Epochs301 and loss 300.2271728515625\n",
            "Epochs302 and loss 300.1903381347656\n",
            "Epochs303 and loss 300.153564453125\n",
            "Epochs304 and loss 300.1167907714844\n",
            "Epochs305 and loss 300.0801086425781\n",
            "Epochs306 and loss 300.04345703125\n",
            "Epochs307 and loss 300.0068359375\n",
            "Epochs308 and loss 299.97027587890625\n",
            "Epochs309 and loss 299.93377685546875\n",
            "Epochs310 and loss 299.8973693847656\n",
            "Epochs311 and loss 299.8609924316406\n",
            "Epochs312 and loss 299.8245544433594\n",
            "Epochs313 and loss 299.78826904296875\n",
            "Epochs314 and loss 299.7520751953125\n",
            "Epochs315 and loss 299.7158203125\n",
            "Epochs316 and loss 299.67962646484375\n",
            "Epochs317 and loss 299.64349365234375\n",
            "Epochs318 and loss 299.60748291015625\n",
            "Epochs319 and loss 299.57147216796875\n",
            "Epochs320 and loss 299.5354919433594\n",
            "Epochs321 and loss 299.49957275390625\n",
            "Epochs322 and loss 299.46368408203125\n",
            "Epochs323 and loss 299.4278869628906\n",
            "Epochs324 and loss 299.39208984375\n",
            "Epochs325 and loss 299.3562927246094\n",
            "Epochs326 and loss 299.3205871582031\n",
            "Epochs327 and loss 299.28497314453125\n",
            "Epochs328 and loss 299.2493896484375\n",
            "Epochs329 and loss 299.2138366699219\n",
            "Epochs330 and loss 299.1783447265625\n",
            "Epochs331 and loss 299.14288330078125\n",
            "Epochs332 and loss 299.10748291015625\n",
            "Epochs333 and loss 299.0721130371094\n",
            "Epochs334 and loss 299.03680419921875\n",
            "Epochs335 and loss 299.00152587890625\n",
            "Epochs336 and loss 298.9663391113281\n",
            "Epochs337 and loss 298.93115234375\n",
            "Epochs338 and loss 298.8960266113281\n",
            "Epochs339 and loss 298.86090087890625\n",
            "Epochs340 and loss 298.825927734375\n",
            "Epochs341 and loss 298.7908935546875\n",
            "Epochs342 and loss 298.7559509277344\n",
            "Epochs343 and loss 298.7210388183594\n",
            "Epochs344 and loss 298.68621826171875\n",
            "Epochs345 and loss 298.65142822265625\n",
            "Epochs346 and loss 298.61663818359375\n",
            "Epochs347 and loss 298.58197021484375\n",
            "Epochs348 and loss 298.5472412109375\n",
            "Epochs349 and loss 298.51263427734375\n",
            "Epochs350 and loss 298.47808837890625\n",
            "Epochs351 and loss 298.4435119628906\n",
            "Epochs352 and loss 298.4090576171875\n",
            "Epochs353 and loss 298.37457275390625\n",
            "Epochs354 and loss 298.3401794433594\n",
            "Epochs355 and loss 298.30584716796875\n",
            "Epochs356 and loss 298.27154541015625\n",
            "Epochs357 and loss 298.2373046875\n",
            "Epochs358 and loss 298.20306396484375\n",
            "Epochs359 and loss 298.16888427734375\n",
            "Epochs360 and loss 298.134765625\n",
            "Epochs361 and loss 298.1007080078125\n",
            "Epochs362 and loss 298.0666809082031\n",
            "Epochs363 and loss 298.0326232910156\n",
            "Epochs364 and loss 297.99871826171875\n",
            "Epochs365 and loss 297.96478271484375\n",
            "Epochs366 and loss 297.93096923828125\n",
            "Epochs367 and loss 297.89715576171875\n",
            "Epochs368 and loss 297.8633728027344\n",
            "Epochs369 and loss 297.8296203613281\n",
            "Epochs370 and loss 297.79595947265625\n",
            "Epochs371 and loss 297.7622985839844\n",
            "Epochs372 and loss 297.7286682128906\n",
            "Epochs373 and loss 297.6952209472656\n",
            "Epochs374 and loss 297.66168212890625\n",
            "Epochs375 and loss 297.6282043457031\n",
            "Epochs376 and loss 297.5947570800781\n",
            "Epochs377 and loss 297.5614318847656\n",
            "Epochs378 and loss 297.528076171875\n",
            "Epochs379 and loss 297.4947204589844\n",
            "Epochs380 and loss 297.4615173339844\n",
            "Epochs381 and loss 297.4283142089844\n",
            "Epochs382 and loss 297.3951721191406\n",
            "Epochs383 and loss 297.36199951171875\n",
            "Epochs384 and loss 297.3289794921875\n",
            "Epochs385 and loss 297.29595947265625\n",
            "Epochs386 and loss 297.26300048828125\n",
            "Epochs387 and loss 297.2300109863281\n",
            "Epochs388 and loss 297.1971130371094\n",
            "Epochs389 and loss 297.16424560546875\n",
            "Epochs390 and loss 297.1314697265625\n",
            "Epochs391 and loss 297.09869384765625\n",
            "Epochs392 and loss 297.06597900390625\n",
            "Epochs393 and loss 297.0332946777344\n",
            "Epochs394 and loss 297.0006408691406\n",
            "Epochs395 and loss 296.9680480957031\n",
            "Epochs396 and loss 296.93548583984375\n",
            "Epochs397 and loss 296.9029541015625\n",
            "Epochs398 and loss 296.8704833984375\n",
            "Epochs399 and loss 296.8380432128906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds=model(inputs)"
      ],
      "metadata": {
        "id": "k0g586GSnSYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoq8E55-n1rS",
        "outputId": "3099573a-55fa-4221-f8f2-8c2fa2dab053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[17.4001, 28.3907],\n",
              "        [35.8373, 41.7625],\n",
              "        [39.0255, 42.3206],\n",
              "        [35.9931, 49.5541],\n",
              "        [33.9906, 38.1261]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8U_aTg9n5cR",
        "outputId": "e7dd027f-4527-4a73-deca-906d92c9084e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[23., 45.],\n",
              "        [45., 65.],\n",
              "        [67., 43.],\n",
              "        [23., 32.],\n",
              "        [12., 21.]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=MSE(target,preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QM4bIYan6SW",
        "outputId": "d86f699f-ca6b-4efd-c4c8-7be41ae7e42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(296.8057, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C9uUnaproIyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}